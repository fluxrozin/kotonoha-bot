# LiteLLM の必要性分析（Claude 専用の場合）

> **注意**: このドキュメントは検討資料です。
> 最終的な決定は
> [ADR-0002: LiteLLM マルチプロバイダー戦略](../architecture/adr/0002-litellm-multi-provider-strategy.md)
> を参照してください。

最初から Claude を使う場合、LiteLLM が必要かどうかを分析します。

## 前提条件

- **開発**: Claude 3 Haiku（レガシー）
- **本番**: Claude Opus 4.5（または Sonnet 4.5）
- **すべて同じプロバイダー（Anthropic）**

## LiteLLM を使う場合のメリット

### 1. フォールバック機能

**シナリオ**: Claude API がダウンした場合

```python
# LiteLLM を使う場合
response = litellm.completion(
    model="anthropic/claude-3-haiku-20240307",
    messages=messages,
    fallbacks=[
        "openai/gpt-3.5-turbo",  # Claude がダウンした場合のフォールバック
        "gemini/gemini-2.5-flash"
    ]
)
```

**メリット**:

- ✅ プロバイダー障害時の自動切り替え
- ✅ 高可用性の確保
- ✅ ユーザーへの影響を最小化

**デメリット**:

- ⚠️ フォールバック先の API キーも必要
- ⚠️ コストが増える可能性

### 2. 将来の拡張性

**シナリオ**: 将来的に新しいプロバイダーを追加したい場合

```python
# 新しいプロバイダーを追加する場合
# LiteLLM を使えば、環境変数の変更のみで対応可能
LLM_MODEL=openai/gpt-4o  # 新しいプロバイダーに簡単に切り替え
```

**メリット**:

- ✅ コード変更なしでプロバイダー追加
- ✅ 統一インターフェースで一貫性を保持
- ✅ 新しいモデルの試用が容易

**デメリット**:

- ⚠️ 現時点では不要な機能かもしれない

### 3. 統一インターフェース

**メリット**:

- ✅ コードの一貫性
- ✅ エラーハンドリングの統一
- ✅ テストの容易さ

**デメリット**:

- ⚠️ 追加の依存関係
- ⚠️ わずかなオーバーヘッド

### 4. コスト管理

**メリット**:

- ✅ 使用量の追跡
- ✅ コスト最適化
- ✅ レート制限の管理

**デメリット**:

- ⚠️ Anthropic SDK でも可能

## LiteLLM を使わない場合（Anthropic SDK を直接使用）

### メリット

1. **シンプルな実装**

   ```python
   from anthropic import Anthropic

   client = Anthropic(api_key=api_key)
   response = client.messages.create(
       model="claude-3-haiku-20240307",
       messages=messages
   )
   ```

2. **依存関係が減る**

   - LiteLLM の依存関係が不要
   - より軽量な実装

3. **直接的な制御**
   - Anthropic SDK の全機能にアクセス
   - より細かい制御が可能

### デメリット

1. **フォールバック機能がない**

   - Claude がダウンした場合の対応が困難
   - 手動での切り替えが必要

2. **将来の拡張性が低い**

   - 新しいプロバイダーを追加する場合、コード変更が必要
   - 抽象化レイヤーを自前で実装する必要がある

3. **統一インターフェースがない**
   - 複数プロバイダーを使う場合、それぞれ異なる実装が必要

## 推奨事項

### 推奨: LiteLLM を残しておく（互換性のため）

**理由**:

1. **フォールバック機能の価値**

   - 本番環境での高可用性が重要
   - Claude がダウンした場合の自動切り替えが可能

2. **将来の拡張性**

   - 新しいプロバイダーやモデルの試用が容易
   - コード変更なしで切り替え可能

3. **統一インターフェース**

   - コードの一貫性と保守性
   - エラーハンドリングの統一

4. **コストが低い**

   - LiteLLM は無料のライブラリ
   - 追加コストは発生しない

5. **既存の設計との整合性**
   - 現在の設計が LiteLLM ベース
   - 変更コストが低い

### 代替案: 条件付きで LiteLLM を使う

**ハイブリッドアプローチ**:

```python
class AIProvider:
    def __init__(self, use_litellm: bool = True):
        self.use_litellm = use_litellm
        if use_litellm:
            # LiteLLM を使用（フォールバック機能あり）
            self.client = "litellm"
        else:
            # Anthropic SDK を直接使用（シンプル）
            from anthropic import Anthropic
            self.client = Anthropic(api_key=api_key)
```

**メリット**:

- ✅ 柔軟性
- ✅ 必要に応じて切り替え可能

**デメリット**:

- ⚠️ 実装が複雑になる
- ⚠️ テストが複雑になる

## 結論

### 推奨: LiteLLM を残しておく

**理由**:

1. **フォールバック機能**: 本番環境での高可用性が重要
2. **将来の拡張性**: 新しいプロバイダーの追加が容易
3. **統一インターフェース**: コードの一貫性と保守性
4. **コスト**: LiteLLM は無料で追加コストなし
5. **既存設計**: 現在の設計との整合性

**ただし、以下の場合は Anthropic SDK を直接使うことも検討可能**:

- フォールバック機能が不要
- 将来の拡張性を考慮しない
- シンプルな実装を優先
- 依存関係を最小限にしたい

### 実装方針

**推奨アプローチ**:

1. **LiteLLM を残す**

   - フォールバック機能を有効にする
   - 統一インターフェースを維持

2. **フォールバック先の設定**

   ```bash
   # .env
   LLM_MODEL=anthropic/claude-3-haiku-20240307
   LLM_FALLBACK_MODEL=openai/gpt-3.5-turbo  # オプション
   ```

3. **段階的な移行**
   - 最初は Claude のみを使用
   - 必要に応じてフォールバック先を追加

## コスト比較

| アプローチ             | 追加コスト             | メリット                                     | デメリット                       |
| ---------------------- | ---------------------- | -------------------------------------------- | -------------------------------- |
| **LiteLLM を使用**     | なし（無料ライブラリ） | フォールバック、拡張性、統一インターフェース | 追加の依存関係                   |
| **Anthropic SDK 直接** | なし                   | シンプル、軽量                               | フォールバックなし、拡張性が低い |

## 最終推奨

**LiteLLM を残しておくことを強く推奨します。**

理由:

- フォールバック機能による高可用性
- 将来の拡張性
- 追加コストなし
- 既存設計との整合性

Claude 専用でも、LiteLLM の価値は十分にあります。

---

**最終更新日**: 2026 年 1 月

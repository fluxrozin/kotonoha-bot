# トークン消費の最適化

## 問題点

LLM で「同じ会話かどうか」を判定する処理が、メッセージが送られるたびに実行されると、トークン消費が激しくなる可能性があります。

### 以前の実装の問題

1. **メッセージごとに LLM API を呼び出す**

   - `should_respond()`が呼ばれるたびに、`_is_same_conversation()`で LLM API を呼び出す
   - 最小間隔チェックを通った場合のみ実行されるが、それでも頻繁に呼び出される可能性がある

2. **同じ会話ログの判定を繰り返す**
   - 同じ会話ログの組み合わせで、何度も LLM 判定を行う
   - キャッシュ機能がないため、無駄な API 呼び出しが発生

## 実装された最適化

### 1. 時間ベースの簡易チェック（最優先）

**実装**:

- 最後の介入から 1 時間以上経過している場合は、明らかに別の会話として扱う
- LLM 判定をスキップし、介入履歴をリセット

**効果**:

- 長時間経過した会話では、LLM API を呼び出さない
- トークン消費を大幅に削減

```python
# 1時間以上経過している場合は、明らかに別の会話として扱う
if time_since_last >= timedelta(hours=1):
    self.intervention_history[channel_id] = []
    return True
```

### 2. キャッシュ機能の追加

**実装**:

- 同じ会話ログの組み合わせの判定結果をキャッシュ
- キャッシュキー: `(チャンネルID, 会話ログのハッシュ)`
- キャッシュの有効期限: 5 分

**効果**:

- 同じ会話ログの組み合わせで、5 分以内は LLM API を呼び出さない
- 短時間内の連続した判定を削減

```python
# キャッシュが有効な場合は再利用
if cache_key in self.conversation_check_cache:
    cached_result, cached_time = self.conversation_check_cache[cache_key]
    if now - cached_time < timedelta(minutes=self.cache_ttl_minutes):
        is_same_conversation = cached_result  # LLM API を呼び出さない
```

### 3. キャッシュの自動クリーンアップ

**実装**:

- 古いキャッシュ（有効期限の 2 倍以上経過）を自動削除
- メモリ使用量を抑制

## 最適化の効果

### トークン消費の削減

1. **キャッシュ機能**
   - 5 分以内の同じ会話ログの判定: **100%削減**（キャッシュから取得）
   - 短時間内の連続した判定を大幅に削減

### 期待される削減率

- **短時間内の連続した判定**: 約 80-90%削減（キャッシュにより）
- **全体**: 約 50-60%のトークン消費削減を期待

**注意**: 時間ベースの簡易チェックは削除しました（Discord では時間が経過していても古いメッセージに返信して会話を続けることがあるため）

## 実装の詳細

### 判定の流れ

```txt
1. 最小間隔チェック（10分）
   ↓ (通過)
2. キャッシュチェック（5分以内）
   ↓ (キャッシュなし/期限切れ)
3. LLM 判定（時間は参考程度、内容を重視）
   ↓
4. 結果をキャッシュに保存
```

### キャッシュの構造

```python
# キー: (チャンネルID, 会話ログのハッシュ)
# 値: (判定結果, キャッシュ時刻)
conversation_check_cache: dict[tuple[int, str], tuple[bool, datetime]] = {}
```

### キャッシュの有効期限

- **デフォルト**: 5 分
- **理由**: 短時間内の連続した判定を削減しつつ、会話の変化に対応できる期間

## 今後の改善案

1. **キャッシュの有効期限の調整**

   - 環境変数で設定可能にする
   - 使用状況に応じて最適な期間を調整

2. **キャッシュサイズの制限**

   - メモリ使用量を抑制
   - LRU キャッシュの採用

3. **統計情報の収集**
   - キャッシュヒット率の追跡
   - トークン消費の削減率の測定

## 注意事項

- キャッシュの有効期限が短すぎると、効果が薄れる
- キャッシュの有効期限が長すぎると、会話の変化に対応できない
- 現在の実装（5 分）は、実用的なバランスを取った設定

## 結論

時間ベースの簡易チェックとキャッシュ機能により、トークン消費を大幅に削減できます。

- **短時間内の連続した判定**: キャッシュにより約 80-90%削減
- **長時間経過した会話**: 時間ベースチェックにより 100%削減
- **全体**: 約 60-70%のトークン消費削減を期待

これにより、LLM で「同じ会話かどうか」を判定する機能を、実用的なコストで運用できます。
